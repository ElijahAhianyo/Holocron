

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>holocron.nn &mdash; holocron 0.2.0.dev0+67a50c7-git documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/js/custom.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/custom_theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="holocron.nn.functional" href="nn.functional.html" />
    <link rel="prev" title="holocron.models" href="models.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> holocron
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installing.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="installing.html#via-python-package">Via Python Package</a></li>
<li class="toctree-l2"><a class="reference internal" href="installing.html#via-conda">Via Conda</a></li>
<li class="toctree-l2"><a class="reference internal" href="installing.html#via-git">Via Git</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notebooks.html">Holocron Notebooks</a></li>
</ul>
<p><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="models.html">holocron.models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="models.html#classification">Classification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="models.html#resnet">ResNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#res2net">Res2Net</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#tridentnet">TridentNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#pyconvresnet">PyConvResNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#rexnet">ReXNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#sknet">SKNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#darknet">Darknet</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#repvgg">RepVGG</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="models.html#object-detection">Object Detection</a><ul>
<li class="toctree-l3"><a class="reference internal" href="models.html#yolo">YOLO</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="models.html#semantic-segmentation">Semantic Segmentation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="models.html#u-net">U-Net</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">holocron.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#non-linear-activations">Non-linear activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#loss-functions">Loss functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#loss-wrappers">Loss wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="#convolution-layers">Convolution layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="#regularization-layers">Regularization layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="#downsampling">Downsampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#attention">Attention</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">holocron.nn.functional</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nn.functional.html#non-linear-activations">Non-linear activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.functional.html#loss-functions">Loss functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.functional.html#convolutions">Convolutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.functional.html#regularization-layers">Regularization layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.functional.html#downsampling">Downsampling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ops.html">holocron.ops</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ops.html#boxes">Boxes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">holocron.optim</a><ul>
<li class="toctree-l2"><a class="reference internal" href="optim.html#optimizers">Optimizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#optimizer-wrappers">Optimizer wrappers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="trainer.html">holocron.trainer</a><ul>
<li class="toctree-l2"><a class="reference internal" href="trainer.html#image-classification">Image classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="trainer.html#miscellaneous">Miscellaneous</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">holocron.utils</a><ul>
<li class="toctree-l2"><a class="reference internal" href="utils.html#miscellaneous">Miscellaneous</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="utils.data.html">holocron.utils.data</a><ul>
<li class="toctree-l2"><a class="reference internal" href="utils.data.html#batch-collate">Batch collate</a></li>
</ul>
</li>
</ul>
<p><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a><ul>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#v0-1-3-2020-10-27">v0.1.3 (2020-10-27)</a></li>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#v0-1-2-2020-06-21">v0.1.2 (2020-06-21)</a></li>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#v0-1-1-2020-05-12">v0.1.1 (2020-05-12)</a></li>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#v0-1-0-2020-05-11">v0.1.0 (2020-05-11)</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">holocron</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>holocron.nn</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/nn.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="holocron-nn">
<h1>holocron.nn<a class="headerlink" href="#holocron-nn" title="Permalink to this headline">¶</a></h1>
<p>An addition to the <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.nn</span></code> module of Pytorch to extend the range of neural networks building blocks.</p>
<section id="non-linear-activations">
<h2>Non-linear activations<a class="headerlink" href="#non-linear-activations" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="holocron.nn.HardMish">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">HardMish</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inplace</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/activation.html#HardMish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.HardMish" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the Had Mish activation module from <a class="reference external" href="https://github.com/digantamisra98/H-Mish">“H-Mish”</a></p>
<p>This activation is computed as follows:</p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{x}{2} \cdot \min(2, \max(0, x + 2))\]</div>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.NLReLU">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">NLReLU</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inplace</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/activation.html#NLReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.NLReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the Natural-Logarithm ReLU activation module from <a class="reference external" href="https://arxiv.org/pdf/1908.03682.pdf">“Natural-Logarithm-Rectified Activation
Function in Convolutional Neural Networks”</a></p>
<p>This activation is computed as follows:</p>
<div class="math notranslate nohighlight">
\[f(x) = ln(1 + \beta \cdot max(0, x))\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inplace</strong> (<em>bool</em>) – should the operation be performed inplace</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.FReLU">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">FReLU</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">kernel_size</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">3</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/activation.html#FReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.FReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the Funnel activation module from <a class="reference external" href="https://arxiv.org/pdf/2007.11824.pdf">“Funnel Activation for Visual Recognition”</a></p>
<p>This activation is computed as follows:</p>
<div class="math notranslate nohighlight">
\[f(x) = max(\mathbb{T}(x), x)\]</div>
<p>where the <span class="math notranslate nohighlight">\(\mathbb{T}\)</span> is the spatial contextual feature extraction. It is a convolution filter of size
<cite>kernel_size</cite>, same padding and groups equal to the number of input channels, followed by a batch normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inplace</strong> (<em>bool</em>) – should the operation be performed inplace</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="loss-functions">
<h2>Loss functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="holocron.nn.FocalLoss">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">FocalLoss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">gamma</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">2.0</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/loss.html#FocalLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.FocalLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of Focal Loss as described in
<a class="reference external" href="https://arxiv.org/pdf/1708.02002.pdf">“Focal Loss for Dense Object Detection”</a>.</p>
<p>While the weighted cross-entropy is described by:</p>
<div class="math notranslate nohighlight">
\[CE(p_t) = -\alpha_t log(p_t)\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha_t\)</span> is the loss weight of class <span class="math notranslate nohighlight">\(t\)</span>,
and <span class="math notranslate nohighlight">\(p_t\)</span> is the predicted probability of class <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>the focal loss introduces a modulating factor</p>
<div class="math notranslate nohighlight">
\[FL(p_t) = -\alpha_t (1 - p_t)^\gamma log(p_t)\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma\)</span> is a positive focusing parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gamma</strong> (<em>float</em><em>, </em><em>optional</em>) – exponent parameter of the focal loss</p></li>
<li><p><strong>weight</strong> (<em>torch.Tensor</em><em>[</em><em>K</em><em>]</em><em>, </em><em>optional</em>) – class weight for loss computation</p></li>
<li><p><strong>ignore_index</strong> (<em>int</em><em>, </em><em>optional</em>) – specifies target value that is ignored and do not contribute to gradient</p></li>
<li><p><strong>reduction</strong> (<em>str</em><em>, </em><em>optional</em>) – type of reduction to apply to the final loss</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.MultiLabelCrossEntropy">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">MultiLabelCrossEntropy</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/loss.html#MultiLabelCrossEntropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.MultiLabelCrossEntropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of the cross-entropy loss for multi-label targets</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<em>torch.Tensor</em><em>[</em><em>K</em><em>]</em><em>, </em><em>optional</em>) – class weight for loss computation</p></li>
<li><p><strong>ignore_index</strong> (<em>int</em><em>, </em><em>optional</em>) – specifies target value that is ignored and do not contribute to gradient</p></li>
<li><p><strong>reduction</strong> (<em>str</em><em>, </em><em>optional</em>) – type of reduction to apply to the final loss</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.ComplementCrossEntropy">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">ComplementCrossEntropy</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">gamma</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/loss.html#ComplementCrossEntropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.ComplementCrossEntropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the complement cross entropy loss from
<a class="reference external" href="https://arxiv.org/pdf/2009.02189.pdf">“Imbalanced Image Classification with Complement Cross Entropy”</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gamma</strong> (<em>float</em><em>, </em><em>optional</em>) – smoothing factor</p></li>
<li><p><strong>weight</strong> (<em>torch.Tensor</em><em>[</em><em>K</em><em>]</em><em>, </em><em>optional</em>) – class weight for loss computation</p></li>
<li><p><strong>ignore_index</strong> (<em>int</em><em>, </em><em>optional</em>) – specifies target value that is ignored and do not contribute to gradient</p></li>
<li><p><strong>reduction</strong> (<em>str</em><em>, </em><em>optional</em>) – type of reduction to apply to the final loss</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.MutualChannelLoss">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">MutualChannelLoss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">weight</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>float<span class="p">, </span>List<span class="p">[</span>float<span class="p">]</span><span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">ignore_index</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">- 100</span></em>, <em class="sig-param"><span class="n">reduction</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'mean'</span></em>, <em class="sig-param"><span class="n">xi</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">2</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/loss.html#MutualChannelLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.MutualChannelLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the mutual channel loss from
<a class="reference external" href="https://arxiv.org/pdf/2002.04264.pdf">“The Devil is in the Channels: Mutual-Channel Loss for Fine-Grained Image Classification”</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<em>torch.Tensor</em><em>[</em><em>K</em><em>]</em><em>, </em><em>optional</em>) – class weight for loss computation</p></li>
<li><p><strong>ignore_index</strong> (<em>int</em><em>, </em><em>optional</em>) – specifies target value that is ignored and do not contribute to gradient</p></li>
<li><p><strong>reduction</strong> (<em>str</em><em>, </em><em>optional</em>) – type of reduction to apply to the final loss</p></li>
<li><p><strong>xi</strong> (<em>in</em><em>, </em><em>optional</em>) – num of features per class</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – diversity factor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.DiceLoss">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">DiceLoss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">weight</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>float<span class="p">, </span>List<span class="p">[</span>float<span class="p">]</span><span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">eps</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1e-08</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/loss.html#DiceLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.DiceLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the dice loss from <a class="reference external" href="https://arxiv.org/pdf/1606.04797.pdf">“V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image
Segmentation”</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<em>torch.Tensor</em><em>[</em><em>K</em><em>]</em><em>, </em><em>optional</em>) – class weight for loss computation</p></li>
<li><p><strong>gamma</strong> (<em>float</em><em>, </em><em>optional</em>) – recall/precision control param</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – small value added to avoid division by zero</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="loss-wrappers">
<h2>Loss wrappers<a class="headerlink" href="#loss-wrappers" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="holocron.nn.ClassBalancedWrapper">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">ClassBalancedWrapper</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">criterion</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">num_samples</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">beta</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.99</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/loss.html#ClassBalancedWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.ClassBalancedWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of the class-balanced loss as described in <a class="reference external" href="https://arxiv.org/pdf/1901.05555.pdf">“Class-Balanced Loss Based on Effective Number
of Samples”</a>.</p>
<p>Given a loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>, the class-balanced loss is described by:</p>
<div class="math notranslate nohighlight">
\[CB(p, y) = \frac{1 - \beta}{1 - \beta^{n_y}} \mathcal{L}(p, y)\]</div>
<p>where <span class="math notranslate nohighlight">\(p\)</span> is the predicted probability for class <span class="math notranslate nohighlight">\(y\)</span>, <span class="math notranslate nohighlight">\(n_y\)</span> is the number of training
samples for class <span class="math notranslate nohighlight">\(y\)</span>, and <span class="math notranslate nohighlight">\(\beta\)</span> is exponential factor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>criterion</strong> (<em>torch.nn.Module</em>) – loss module</p></li>
<li><p><strong>num_samples</strong> (<em>torch.Tensor</em><em>[</em><em>K</em><em>]</em>) – number of samples for each class</p></li>
<li><p><strong>beta</strong> (<em>float</em><em>, </em><em>optional</em>) – rebalancing exponent</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="convolution-layers">
<h2>Convolution layers<a class="headerlink" href="#convolution-layers" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="holocron.nn.NormConv2d">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">NormConv2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">out_channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">kernel_size</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">stride</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">padding</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">dilation</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">groups</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">bias</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">padding_mode</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'zeros'</span></em>, <em class="sig-param"><span class="n">eps</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1e-14</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/conv.html#NormConv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.NormConv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the normalized convolution module from <a class="reference external" href="https://arxiv.org/pdf/2005.05274v2.pdf">“Normalized Convolutional Neural Network”</a>.</p>
<p>In the simplest case, the output value of the layer with input size
<span class="math notranslate nohighlight">\((N, C_{in}, H, W)\)</span> and output <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>
can be precisely described as:</p>
<div class="math notranslate nohighlight">
\[out(N_i, C_{out_j}) = bias(C_{out_j}) +
\sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star
\frac{input(N_i, k) - \mu(N_i, k)}{\sqrt{\sigma^2(N_i, k) + \epsilon}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\star\)</span> is the valid 2D cross-correlation operator,
<span class="math notranslate nohighlight">\(\mu(N_i, k)\)</span> and <span class="math notranslate nohighlight">\(\sigma²(N_i, k)\)</span> are the mean and variance of <span class="math notranslate nohighlight">\(input(N_i, k)\)</span> over all slices,
<span class="math notranslate nohighlight">\(N\)</span> is a batch size, <span class="math notranslate nohighlight">\(C\)</span> denotes a number of channels,
<span class="math notranslate nohighlight">\(H\)</span> is a height of input planes in pixels, and <span class="math notranslate nohighlight">\(W\)</span> is
width in pixels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Zero-padding added to both sides of
the input. Default: 0</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Spacing between kernel
elements. Default: 1</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of blocked connections from input
channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the
output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code></p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – a value added to the denominator for numerical stability.
Default: 1e-14</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.Add2d">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">Add2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">out_channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">kernel_size</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">stride</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">padding</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">dilation</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">groups</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">bias</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">padding_mode</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'zeros'</span></em>, <em class="sig-param"><span class="n">normalize_slices</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">eps</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1e-14</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/conv.html#Add2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.Add2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the adder module from <a class="reference external" href="https://arxiv.org/pdf/1912.13200.pdf">“AdderNet: Do We Really Need Multiplications in Deep Learning?”</a>.</p>
<p>In the simplest case, the output value of the layer at position <span class="math notranslate nohighlight">\((m, n)\)</span> in channel <span class="math notranslate nohighlight">\(c\)</span>
with filter F of spatial size <span class="math notranslate nohighlight">\((d, d)\)</span>, intput size <span class="math notranslate nohighlight">\((C_{in}, H, W)\)</span> and output <span class="math notranslate nohighlight">\((C_{out}, H, W)\)</span>
can be precisely described as:</p>
<div class="math notranslate nohighlight">
\[out(m, n, c) = - \sum\limits_{i=0}^d \sum\limits_{j=0}^d \sum\limits_{k=0}^{C_{in}}
|X(m + i, n + j, k) - F(i, j, k, c)|\]</div>
<p>where <span class="math notranslate nohighlight">\(C\)</span> denotes a number of channels,
<span class="math notranslate nohighlight">\(H\)</span> is a height of input planes in pixels, and <span class="math notranslate nohighlight">\(W\)</span> is
width in pixels.</p>
<img alt="Add2D schema" class="align-center" src="https://github.com/frgfm/Holocron/releases/download/v0.1.3/add2d.png" />
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Zero-padding added to both sides of
the input. Default: 0</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Spacing between kernel
elements. Default: 1</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of blocked connections from input
channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the
output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code></p></li>
<li><p><strong>normalize_slices</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether slices should be normalized before performing cross-correlation.
Default: False</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – a value added to the denominator for numerical stability.
Default: 1e-14</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.SlimConv2d">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">SlimConv2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">kernel_size</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">stride</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">padding</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">dilation</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">groups</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">bias</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">padding_mode</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'zeros'</span></em>, <em class="sig-param"><span class="n">r</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">32</span></em>, <em class="sig-param"><span class="n">L</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">2</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/conv.html#SlimConv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.SlimConv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the convolution module from <a class="reference external" href="https://arxiv.org/pdf/2003.07469.pdf">“SlimConv: Reducing Channel Redundancy in Convolutional Neural Networks
by Weights Flipping”</a>.</p>
<p>First, we compute channel-wise weights as follows:</p>
<div class="math notranslate nohighlight">
\[z(c) = \frac{1}{H \cdot W} \sum\limits_{i=1}^H \sum\limits_{j=1}^W X_{c,i,j}\]</div>
<p>where <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{C \times H \times W}\)</span> is the input tensor,
<span class="math notranslate nohighlight">\(H\)</span> is height in pixels, and <span class="math notranslate nohighlight">\(W\)</span> is
width in pixels.</p>
<div class="math notranslate nohighlight">
\[w = \sigma(F_{fc2}(\delta(F_{fc1}(z))))\]</div>
<p>where <span class="math notranslate nohighlight">\(z \in \mathbb{R}^{C}\)</span> contains channel-wise statistics,
<span class="math notranslate nohighlight">\(\sigma\)</span> refers to the sigmoid function,
<span class="math notranslate nohighlight">\(\delta\)</span> refers to the ReLU function,
<span class="math notranslate nohighlight">\(F_{fc1}\)</span> is a convolution operation with kernel of size <span class="math notranslate nohighlight">\((1, 1)\)</span>
with <span class="math notranslate nohighlight">\(max(C/r, L)\)</span> output channels followed by batch normalization,
and <span class="math notranslate nohighlight">\(F_{fc2}\)</span> is a plain convolution operation with kernel of size <span class="math notranslate nohighlight">\((1, 1)\)</span>
with <span class="math notranslate nohighlight">\(C\)</span> output channels.</p>
<p>We then proceed with reconstructing and transforming both pathways:</p>
<div class="math notranslate nohighlight">
\[X_{top} = X \odot w\]</div>
<div class="math notranslate nohighlight">
\[X_{bot} = X \odot \check{w}\]</div>
<p>where <span class="math notranslate nohighlight">\(\odot\)</span> refers to the element-wise multiplication and <span class="math notranslate nohighlight">\(\check{w}\)</span> is
the channel-wise reverse-flip of <span class="math notranslate nohighlight">\(w\)</span>.</p>
<div class="math notranslate nohighlight">
\[T_{top} = F_{top}(X_{top}^{(1)} + X_{top}^{(2)})\]</div>
<div class="math notranslate nohighlight">
\[T_{bot} = F_{bot}(X_{bot}^{(1)} + X_{bot}^{(2)})\]</div>
<p>where <span class="math notranslate nohighlight">\(X^{(1)}\)</span> and <span class="math notranslate nohighlight">\(X^{(2)}\)</span> are the channel-wise first and second halves of <span class="math notranslate nohighlight">\(X\)</span>,
<span class="math notranslate nohighlight">\(F_{top}\)</span> is a convolution of kernel size <span class="math notranslate nohighlight">\((3, 3)\)</span>,
and <span class="math notranslate nohighlight">\(F_{bot}\)</span> is a convolution of kernel size <span class="math notranslate nohighlight">\((1, 1)\)</span> reducing channels by half,
followed by a convolution of kernel size <span class="math notranslate nohighlight">\((3, 3)\)</span>.</p>
<p>Finally we fuse both pathways to yield the output:</p>
<div class="math notranslate nohighlight">
\[Y = T_{top} \oplus T_{bot}\]</div>
<p>where <span class="math notranslate nohighlight">\(\oplus\)</span> is the channel-wise concatenation.</p>
<img alt="SlimConv2D schema" class="align-center" src="https://github.com/frgfm/Holocron/releases/download/v0.1.3/slimconv2d.png" />
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input image</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Zero-padding added to both sides of
the input. Default: 0</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Spacing between kernel
elements. Default: 1</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of blocked connections from input
channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the
output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code></p></li>
<li><p><strong>r</strong> (<em>int</em><em>, </em><em>optional</em>) – squeezing divider. Default: 32</p></li>
<li><p><strong>L</strong> (<em>int</em><em>, </em><em>optional</em>) – minimum squeezed channels. Default: 8</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.PyConv2d">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">PyConv2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">out_channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">kernel_size</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">num_levels</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">2</span></em>, <em class="sig-param"><span class="n">padding</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">groups</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/conv.html#PyConv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.PyConv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the convolution module from <a class="reference external" href="https://arxiv.org/pdf/2006.11538.pdf">“Pyramidal Convolution: Rethinking Convolutional Neural Networks for
Visual Recognition”</a>.</p>
<img alt="PyConv2D schema" class="align-center" src="https://github.com/frgfm/Holocron/releases/download/v0.1.3/pyconv2d.png" />
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em>) – Size of the convolving kernel</p></li>
<li><p><strong>num_levels</strong> (<em>int</em><em>, </em><em>optional</em>) – number of stacks in the pyramid</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Zero-padding added to both sides of
the input. Default: 0</p></li>
<li><p><strong>groups</strong> (<em>list</em><em>(</em><em>int</em><em>)</em><em>, </em><em>optional</em>) – Number of blocked connections from input
channels to output channels. Default: 1</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.Involution2d">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">Involution2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">kernel_size</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">padding</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">stride</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">groups</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">dilation</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">reduction_ratio</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/conv.html#Involution2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.Involution2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the convolution module from <a class="reference external" href="https://arxiv.org/pdf/2103.06255.pdf">“Involution: Inverting the Inherence of Convolution for Visual
Recognition”</a>, adapted from the proposed PyTorch implementation in
the paper.</p>
<img alt="Involution2d schema" class="align-center" src="https://github.com/frgfm/Holocron/releases/download/v0.1.3/involutions.png" />
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input image</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em>) – Size of the convolving kernel</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Zero-padding added to both sides of
the input. Default: 0</p></li>
<li><p><strong>stride</strong> – Stride of the convolution. Default: 1</p></li>
<li><p><strong>groups</strong> – Number of blocked connections from input channels to output channels. Default: 1</p></li>
<li><p><strong>dilation</strong> – Spacing between kernel elements. Default: 1</p></li>
<li><p><strong>reduction_ratio</strong> – reduction ratio of the channels to generate the kernel</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="regularization-layers">
<h2>Regularization layers<a class="headerlink" href="#regularization-layers" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="holocron.nn.DropBlock2d">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">DropBlock2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">block_size</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">7</span></em>, <em class="sig-param"><span class="n">inplace</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/dropblock.html#DropBlock2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.DropBlock2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the DropBlock module from <a class="reference external" href="https://arxiv.org/pdf/1810.12890.pdf">“DropBlock: A regularization method for convolutional networks”</a></p>
<img alt="https://github.com/frgfm/Holocron/releases/download/v0.1.3/dropblock.png" class="align-center" src="https://github.com/frgfm/Holocron/releases/download/v0.1.3/dropblock.png" />
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> (<em>float</em><em>, </em><em>optional</em>) – probability of dropping activation value</p></li>
<li><p><strong>block_size</strong> (<em>int</em><em>, </em><em>optional</em>) – size of each block that is expended from the sampled mask</p></li>
<li><p><strong>inplace</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether the operation should be done inplace</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="downsampling">
<h2>Downsampling<a class="headerlink" href="#downsampling" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="holocron.nn.ConcatDownsample2d">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">ConcatDownsample2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">scale_factor</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/downsample.html#ConcatDownsample2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.ConcatDownsample2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements a loss-less downsampling operation described in <a class="reference external" href="https://pjreddie.com/media/files/papers/YOLO9000.pdf">“YOLO9000: Better, Faster, Stronger”</a> by stacking adjacent information on the channel dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>scale_factor</strong> (<em>int</em>) – spatial scaling factor</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.GlobalAvgPool2d">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">GlobalAvgPool2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">flatten</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/downsample.html#GlobalAvgPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.GlobalAvgPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Fast implementation of global average pooling from <a class="reference external" href="https://arxiv.org/pdf/2003.13630.pdf">“TResNet: High Performance GPU-Dedicated Architecture”</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>flatten</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether spatial dimensions should be squeezed</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.BlurPool2d">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">BlurPool2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">kernel_size</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">3</span></em>, <em class="sig-param"><span class="n">stride</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">2</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/downsample.html#BlurPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.BlurPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Ross Wightman’s <a class="reference external" href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/blur_pool.py">implementation</a> of blur pooling
module as described in <a class="reference external" href="https://arxiv.org/pdf/1904.11486.pdf">“Making Convolutional Networks Shift-Invariant Again”</a>.</p>
<img alt="https://github.com/frgfm/Holocron/releases/download/v0.1.3/blurpool.png" class="align-center" src="https://github.com/frgfm/Holocron/releases/download/v0.1.3/blurpool.png" />
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>channels</strong> (<em>int</em>) – Number of input channels</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em>) – binomial filter size for blurring. currently supports 3 (default) and 5.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em>, </em><em>optional</em>) – downsampling filter stride</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the transformed tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.SPP">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">SPP</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">kernel_sizes</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/downsample.html#SPP"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.SPP" title="Permalink to this definition">¶</a></dt>
<dd><p>SPP layer from <a class="reference external" href="https://arxiv.org/pdf/1406.4729.pdf">“Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition”</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>kernel_sizes</strong> (<em>list&lt;int&gt;</em>) – kernel sizes of each pooling</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.ZPool">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">ZPool</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dim</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/downsample.html#ZPool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.ZPool" title="Permalink to this definition">¶</a></dt>
<dd><p>Z-pool layer from <a class="reference external" href="https://arxiv.org/pdf/2010.03045.pdf">“Rotate to Attend: Convolutional Triplet Attention Module”</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dim</strong> – dimension to pool</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="attention">
<h2>Attention<a class="headerlink" href="#attention" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="holocron.nn.SAM">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">SAM</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_channels</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/attention.html#SAM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.SAM" title="Permalink to this definition">¶</a></dt>
<dd><p>SAM layer from <a class="reference external" href="https://arxiv.org/pdf/1807.06521.pdf">“CBAM: Convolutional Block Attention Module”</a>
modified in <a class="reference external" href="https://arxiv.org/pdf/2004.10934.pdf">“YOLOv4: Optimal Speed and Accuracy of Object Detection”</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>in_channels</strong> (<em>int</em>) – input channels</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.LambdaLayer">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">LambdaLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">out_channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">dim_k</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">n</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">r</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_heads</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">4</span></em>, <em class="sig-param"><span class="n">dim_u</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/lambda_layer.html#LambdaLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.LambdaLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Lambda layer from <a class="reference external" href="https://openreview.net/pdf?id=xTJEN-ggl1b">“LambdaNetworks: Modeling long-range interactions without attention”</a>. The implementation was adapted from <a class="reference external" href="https://github.com/lucidrains/lambda-networks/blob/main/lambda_networks/lambda_networks.py">lucidrains’</a>.</p>
<img alt="https://github.com/frgfm/Holocron/releases/download/v0.1.3/lambdalayer.png" class="align-center" src="https://github.com/frgfm/Holocron/releases/download/v0.1.3/lambdalayer.png" />
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – input channels</p></li>
<li><p><strong>out_channels</strong> (<em>int</em><em>, </em><em>optional</em>) – output channels</p></li>
<li><p><strong>dim_k</strong> (<em>int</em>) – key dimension</p></li>
<li><p><strong>n</strong> (<em>int</em><em>, </em><em>optional</em>) – number of input pixels</p></li>
<li><p><strong>r</strong> (<em>int</em><em>, </em><em>optional</em>) – receptive field for relative positional encoding</p></li>
<li><p><strong>num_heads</strong> (<em>int</em><em>, </em><em>optional</em>) – number of attention heads</p></li>
<li><p><strong>dim_u</strong> (<em>int</em><em>, </em><em>optional</em>) – intra-depth dimension</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.TripletAttention">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">TripletAttention</code><a class="reference internal" href="_modules/holocron/nn/modules/attention.html#TripletAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.TripletAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Triplet attention layer from <a class="reference external" href="https://arxiv.org/pdf/2010.03045.pdf">“Rotate to Attend: Convolutional Triplet Attention Module”</a>. This implementation is based on the
<a class="reference external" href="https://github.com/LandskapeAI/triplet-attention/blob/master/MODELS/triplet_attention.py">one</a>
from the paper’s authors.</p>
</dd></dl>

</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="nn.functional.html" class="btn btn-neutral float-right" title="holocron.nn.functional" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="models.html" class="btn btn-neutral float-left" title="holocron.models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019-2022, François-Guillaume Fernandez

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-148140560-2', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>