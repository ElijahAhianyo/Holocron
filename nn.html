

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>holocron.nn &mdash; holocron 0.1.2a0+a786789-git documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  <link rel="stylesheet" href="_static/css/my_theme.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="holocron.nn.functional" href="nn.functional.html" />
    <link rel="prev" title="holocron.models" href="models.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> holocron
          

          
          </a>

          
            
            
              <div class="version">
                0.1.2a0+a786789
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installing.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="installing.html#via-python-package">Via Python Package</a></li>
<li class="toctree-l2"><a class="reference internal" href="installing.html#via-conda">Via Conda</a></li>
<li class="toctree-l2"><a class="reference internal" href="installing.html#via-git">Via Git</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="models.html">holocron.models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="models.html#classification">Classification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="models.html#res2net">Res2Net</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#res2next">Res2NeXt</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#darknet">Darknet</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="models.html#object-detection">Object Detection</a><ul>
<li class="toctree-l3"><a class="reference internal" href="models.html#yolo">YOLO</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="models.html#semantic-segmentation">Semantic Segmentation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="models.html#u-net">U-Net</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">holocron.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#non-linear-activations">Non-linear activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#loss-functions">Loss functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#loss-wrappers">Loss wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="#convolution-layers">Convolution layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="#regularization-layers">Regularization layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="#downsampling">Downsampling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">holocron.nn.functional</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nn.functional.html#non-linear-activations">Non-linear activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.functional.html#loss-functions">Loss functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.functional.html#convolutions">Convolutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.functional.html#regularization-layers">Regularization layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.functional.html#downsampling">Downsampling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ops.html">holocron.ops</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ops.html#boxes">Boxes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">holocron.optim</a><ul>
<li class="toctree-l2"><a class="reference internal" href="optim.html#optimizers">Optimizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#optimizer-wrappers">Optimizer wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#learning-rate-schedulers">Learning rate schedulers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">holocron.utils</a><ul>
<li class="toctree-l2"><a class="reference internal" href="utils.html#miscellaneous">Miscellaneous</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="utils.data.html">holocron.utils.data</a><ul>
<li class="toctree-l2"><a class="reference internal" href="utils.data.html#batch-collate">Batch collate</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">holocron</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>holocron.nn</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/nn.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="holocron-nn">
<h1>holocron.nn<a class="headerlink" href="#holocron-nn" title="Permalink to this headline">¶</a></h1>
<p>An addition to the <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.nn</span></code> module of Pytorch to extend the range of neural networks building blocks.</p>
<div class="section" id="non-linear-activations">
<h2>Non-linear activations<a class="headerlink" href="#non-linear-activations" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="holocron.nn.Mish">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">Mish</code><a class="reference internal" href="_modules/holocron/nn/modules/activation.html#Mish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.Mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the Mish activation module from <a class="reference external" href="https://arxiv.org/pdf/1908.08681.pdf">“Mish: A Self Regularized Non-Monotonic Neural Activation Function”</a></p>
<p>This activation is computed as follows:</p>
<div class="math notranslate nohighlight">
\[f(x) = x \cdot \tanh(ln(1 + e^x))\]</div>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.NLReLU">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">NLReLU</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inplace</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/activation.html#NLReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.NLReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the Natural-Logarithm ReLU activation module from <a class="reference external" href="https://arxiv.org/pdf/1908.03682.pdf">“Natural-Logarithm-Rectified Activation
Function in Convolutional Neural Networks”</a></p>
<p>This activation is computed as follows:</p>
<div class="math notranslate nohighlight">
\[f(x) = ln(1 + \beta \cdot max(0, x))\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inplace</strong> (<em>bool</em>) – should the operation be performed inplace</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="loss-functions">
<h2>Loss functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="holocron.nn.FocalLoss">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">FocalLoss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">gamma</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/loss.html#FocalLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.FocalLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of Focal Loss as described in
<a class="reference external" href="https://arxiv.org/pdf/1708.02002.pdf">“Focal Loss for Dense Object Detection”</a>.</p>
<p>While the weighted cross-entropy is described by:</p>
<div class="math notranslate nohighlight">
\[CE(p_t) = -\alpha_t log(p_t)\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha_t\)</span> is the loss weight of class <span class="math notranslate nohighlight">\(t\)</span>,
and <span class="math notranslate nohighlight">\(p_t\)</span> is the predicted probability of class <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>the focal loss introduces a modulating factor</p>
<div class="math notranslate nohighlight">
\[FL(p_t) = -\alpha_t (1 - p_t)^\gamma log(p_t)\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma\)</span> is a positive focusing parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gamma</strong> (<em>python:float</em><em>, </em><em>optional</em>) – exponent parameter of the focal loss</p></li>
<li><p><strong>weight</strong> (<em>torch.Tensor</em><em>[</em><em>K</em><em>]</em><em>, </em><em>optional</em>) – class weight for loss computation</p></li>
<li><p><strong>ignore_index</strong> (<em>python:int</em><em>, </em><em>optional</em>) – specifies target value that is ignored and do not contribute to gradient</p></li>
<li><p><strong>reduction</strong> (<em>str</em><em>, </em><em>optional</em>) – type of reduction to apply to the final loss</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.MultiLabelCrossEntropy">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">MultiLabelCrossEntropy</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/loss.html#MultiLabelCrossEntropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.MultiLabelCrossEntropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of the cross-entropy loss for multi-label targets</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<em>torch.Tensor</em><em>[</em><em>K</em><em>]</em><em>, </em><em>optional</em>) – class weight for loss computation</p></li>
<li><p><strong>ignore_index</strong> (<em>python:int</em><em>, </em><em>optional</em>) – specifies target value that is ignored and do not contribute to gradient</p></li>
<li><p><strong>reduction</strong> (<em>str</em><em>, </em><em>optional</em>) – type of reduction to apply to the final loss</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.LabelSmoothingCrossEntropy">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">LabelSmoothingCrossEntropy</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/loss.html#LabelSmoothingCrossEntropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.LabelSmoothingCrossEntropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of the cross-entropy loss with label smoothing on hard target as described in
<a class="reference external" href="https://arxiv.org/pdf/1706.03762.pdf">“Attention Is All You Need”</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>eps</strong> (<em>python:float</em><em>, </em><em>optional</em>) – smoothing factor</p></li>
<li><p><strong>weight</strong> (<em>torch.Tensor</em><em>[</em><em>K</em><em>]</em><em>, </em><em>optional</em>) – class weight for loss computation</p></li>
<li><p><strong>ignore_index</strong> (<em>python:int</em><em>, </em><em>optional</em>) – specifies target value that is ignored and do not contribute to gradient</p></li>
<li><p><strong>reduction</strong> (<em>str</em><em>, </em><em>optional</em>) – type of reduction to apply to the final loss</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="loss-wrappers">
<h2>Loss wrappers<a class="headerlink" href="#loss-wrappers" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="holocron.nn.MixupLoss">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">MixupLoss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">criterion</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/loss.html#MixupLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.MixupLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements a Mixup wrapper as described in
<a class="reference external" href="https://arxiv.org/pdf/1710.09412.pdf">“mixup: Beyond Empirical Risk Minimization”</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>criterion</strong> (<em>callable</em>) – initial criterion to be used on normal sample &amp; targets</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="convolution-layers">
<h2>Convolution layers<a class="headerlink" href="#convolution-layers" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="holocron.nn.NormConv2d">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">NormConv2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_channels</span></em>, <em class="sig-param"><span class="n">out_channels</span></em>, <em class="sig-param"><span class="n">kernel_size</span></em>, <em class="sig-param"><span class="n">stride</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">padding</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">dilation</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">groups</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">bias</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">padding_mode</span><span class="o">=</span><span class="default_value">'zeros'</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">1e-14</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/conv.html#NormConv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.NormConv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the normalized convolution module from <a class="reference external" href="https://arxiv.org/pdf/2005.05274v2.pdf">“Normalized Convolutional Neural Network”</a>.</p>
<p>In the simplest case, the output value of the layer with input size
<span class="math notranslate nohighlight">\((N, C_{in}, H, W)\)</span> and output <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>
can be precisely described as:</p>
<div class="math notranslate nohighlight">
\[out(N_i, C_{out_j}) = bias(C_{out_j}) +
\sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star
\frac{input(N_i, k) - \mu(N_i, k)}{\sqrt{\sigma^2(N_i, k) + \epsilon}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\star\)</span> is the valid 2D cross-correlation operator,
<span class="math notranslate nohighlight">\(\mu(N_i, k)\)</span> and <span class="math notranslate nohighlight">\(\sigma²(N_i, k)\)</span> are the mean and variance of <span class="math notranslate nohighlight">\(input(N_i, k)\)</span> over all slices,
<span class="math notranslate nohighlight">\(N\)</span> is a batch size, <span class="math notranslate nohighlight">\(C\)</span> denotes a number of channels,
<span class="math notranslate nohighlight">\(H\)</span> is a height of input planes in pixels, and <span class="math notranslate nohighlight">\(W\)</span> is
width in pixels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>python:int</em>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<em>python:int</em>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<em>python:int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<em>python:int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<em>python:int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Zero-padding added to both sides of
the input. Default: 0</p></li>
<li><p><strong>dilation</strong> (<em>python:int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Spacing between kernel
elements. Default: 1</p></li>
<li><p><strong>groups</strong> (<em>python:int</em><em>, </em><em>optional</em>) – Number of blocked connections from input
channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the
output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code></p></li>
<li><p><strong>eps</strong> (<em>python:float</em><em>, </em><em>optional</em>) – a value added to the denominator for numerical stability.
Default: 1e-14</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.Add2d">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">Add2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_channels</span></em>, <em class="sig-param"><span class="n">out_channels</span></em>, <em class="sig-param"><span class="n">kernel_size</span></em>, <em class="sig-param"><span class="n">stride</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">padding</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">dilation</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">groups</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">bias</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">padding_mode</span><span class="o">=</span><span class="default_value">'zeros'</span></em>, <em class="sig-param"><span class="n">normalize_slices</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">1e-14</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/conv.html#Add2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.Add2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the adder module from <a class="reference external" href="https://arxiv.org/pdf/1912.13200.pdf">“AdderNet: Do We Really Need Multiplications in Deep Learning?”</a>.</p>
<p>In the simplest case, the output value of the layer at position <span class="math notranslate nohighlight">\((m, n)\)</span> in channel <span class="math notranslate nohighlight">\(c\)</span>
with filter F of spatial size <span class="math notranslate nohighlight">\((d, d)\)</span>, intput size <span class="math notranslate nohighlight">\((C_{in}, H, W)\)</span> and output <span class="math notranslate nohighlight">\((C_{out}, H, W)\)</span>
can be precisely described as:</p>
<div class="math notranslate nohighlight">
\[out(m, n, c) = - \sum\limits_{i=0}^d \sum\limits_{j=0}^d \sum\limits_{k=0}^{C_{in}}
|X(m + i, n + j, k) - F(i, j, k, c)|\]</div>
<p>where <span class="math notranslate nohighlight">\(C\)</span> denotes a number of channels,
<span class="math notranslate nohighlight">\(H\)</span> is a height of input planes in pixels, and <span class="math notranslate nohighlight">\(W\)</span> is
width in pixels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>python:int</em>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<em>python:int</em>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<em>python:int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<em>python:int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<em>python:int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Zero-padding added to both sides of
the input. Default: 0</p></li>
<li><p><strong>dilation</strong> (<em>python:int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Spacing between kernel
elements. Default: 1</p></li>
<li><p><strong>groups</strong> (<em>python:int</em><em>, </em><em>optional</em>) – Number of blocked connections from input
channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the
output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code></p></li>
<li><p><strong>normalize_slices</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether slices should be normalized before performing cross-correlation.
Default: False</p></li>
<li><p><strong>eps</strong> (<em>python:float</em><em>, </em><em>optional</em>) – a value added to the denominator for numerical stability.
Default: 1e-14</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="holocron.nn.SlimConv2d">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">SlimConv2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_channels</span></em>, <em class="sig-param"><span class="n">kernel_size</span></em>, <em class="sig-param"><span class="n">stride</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">padding</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">dilation</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">groups</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">bias</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">padding_mode</span><span class="o">=</span><span class="default_value">'zeros'</span></em>, <em class="sig-param"><span class="n">r</span><span class="o">=</span><span class="default_value">32</span></em>, <em class="sig-param"><span class="n">L</span><span class="o">=</span><span class="default_value">2</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/conv.html#SlimConv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.SlimConv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the convolution module from <a class="reference external" href="https://arxiv.org/pdf/2003.07469.pdf">“SlimConv: Reducing Channel Redundancy in Convolutional Neural Networks
by Weights Flipping”</a>.</p>
<p>First, we compute channel-wise weights as follows:</p>
<div class="math notranslate nohighlight">
\[z(c) = \frac{1}{H \cdot W} \sum\limits_{i=1}^H \sum\limits_{j=1}^W X_{c,i,j}\]</div>
<p>where <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{C \times H \times W}\)</span> is the input tensor,
<span class="math notranslate nohighlight">\(H\)</span> is height in pixels, and <span class="math notranslate nohighlight">\(W\)</span> is
width in pixels.</p>
<div class="math notranslate nohighlight">
\[w = \sigma(F_{fc2}(\delta(F_{fc1}(z))))\]</div>
<p>where <span class="math notranslate nohighlight">\(z \in \mathbb{R}^{C}\)</span> contains channel-wise statistics,
<span class="math notranslate nohighlight">\(\sigma\)</span> refers to the sigmoid function,
<span class="math notranslate nohighlight">\(\delta\)</span> refers to the ReLU function,
<span class="math notranslate nohighlight">\(F_{fc1}\)</span> is a convolution operation with kernel of size <span class="math notranslate nohighlight">\((1, 1)\)</span>
with <span class="math notranslate nohighlight">\(max(C/r, L)\)</span> output channels followed by batch normalization,
and <span class="math notranslate nohighlight">\(F_{fc2}\)</span> is a plain convolution operation with kernel of size <span class="math notranslate nohighlight">\((1, 1)\)</span>
with <span class="math notranslate nohighlight">\(C\)</span> output channels.</p>
<p>We then proceed with reconstructing and transforming both pathways:</p>
<div class="math notranslate nohighlight">
\[X_{top} = X \odot w\]</div>
<div class="math notranslate nohighlight">
\[X_{bot} = X \odot \check{w}\]</div>
<p>where <span class="math notranslate nohighlight">\(\odot\)</span> refers to the element-wise multiplication and <span class="math notranslate nohighlight">\(\check{w}\)</span> is
the channel-wise reverse-flip of <span class="math notranslate nohighlight">\(w\)</span>.</p>
<div class="math notranslate nohighlight">
\[T_{top} = F_{top}(X_{top}^{(1)} + X_{top}^{(2)})\]</div>
<div class="math notranslate nohighlight">
\[T_{bot} = F_{bot}(X_{bot}^{(1)} + X_{bot}^{(2)})\]</div>
<p>where <span class="math notranslate nohighlight">\(X^{(1)}\)</span> and <span class="math notranslate nohighlight">\(X^{(2)}\)</span> are the channel-wise first and second halves of <span class="math notranslate nohighlight">\(X\)</span>,
<span class="math notranslate nohighlight">\(F_{top}\)</span> is a convolution of kernel size <span class="math notranslate nohighlight">\((3, 3)\)</span>,
and <span class="math notranslate nohighlight">\(F_{bot}\)</span> is a convolution of kernel size <span class="math notranslate nohighlight">\((1, 1)\)</span> reducing channels by half,
followed by a convolution of kernel size <span class="math notranslate nohighlight">\((3, 3)\)</span>.</p>
<p>Finally we fuse both pathways to yield the output:</p>
<div class="math notranslate nohighlight">
\[Y = T_{top} \oplus T_{bot}\]</div>
<p>where <span class="math notranslate nohighlight">\(\oplus\)</span> is the channel-wise concatenation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>python:int</em>) – Number of channels in the input image</p></li>
<li><p><strong>kernel_size</strong> (<em>python:int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<em>python:int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<em>python:int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Zero-padding added to both sides of
the input. Default: 0</p></li>
<li><p><strong>dilation</strong> (<em>python:int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Spacing between kernel
elements. Default: 1</p></li>
<li><p><strong>groups</strong> (<em>python:int</em><em>, </em><em>optional</em>) – Number of blocked connections from input
channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the
output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code></p></li>
<li><p><strong>r</strong> (<em>python:int</em><em>, </em><em>optional</em>) – squeezing divider. Default: 32</p></li>
<li><p><strong>L</strong> (<em>python:int</em><em>, </em><em>optional</em>) – minimum squeezed channels. Default: 8</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="regularization-layers">
<h2>Regularization layers<a class="headerlink" href="#regularization-layers" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="holocron.nn.DropBlock2d">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">DropBlock2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em>, <em class="sig-param"><span class="n">block_size</span></em>, <em class="sig-param"><span class="n">inplace</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/dropblock.html#DropBlock2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.DropBlock2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the DropBlock module from <a class="reference external" href="https://arxiv.org/pdf/1810.12890.pdf">“DropBlock: A regularization method for convolutional networks”</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> (<em>python:float</em>) – probability of dropping activation value</p></li>
<li><p><strong>block_size</strong> (<em>python:int</em>) – size of each block that is expended from the sampled mask</p></li>
<li><p><strong>inplace</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether the operation should be done inplace</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="downsampling">
<h2>Downsampling<a class="headerlink" href="#downsampling" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="holocron.nn.ConcatDownsample2d">
<em class="property">class </em><code class="sig-prename descclassname">holocron.nn.</code><code class="sig-name descname">ConcatDownsample2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">scale_factor</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/holocron/nn/modules/downsample.html#ConcatDownsample2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#holocron.nn.ConcatDownsample2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements a loss-less downsampling operation described in <a class="reference external" href="https://pjreddie.com/media/files/papers/YOLO9000.pdf">“YOLO9000: Better, Faster, Stronger”</a> by stacking adjacent information on the channel dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>scale_factor</strong> (<em>python:int</em>) – spatial scaling factor</p>
</dd>
</dl>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="nn.functional.html" class="btn btn-neutral float-right" title="holocron.nn.functional" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="models.html" class="btn btn-neutral float-left" title="holocron.models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, François-Guillaume Fernandez

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-148140560-2', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>